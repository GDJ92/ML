x,y
1.KNNREGRESSOR Scratch,"from sklearn.datasets import fetch_california_housing ; import pandas as pd ; import numpy as np / housing = fetch_california_housing() ; df = pd.DataFrame(housing.data, columns = housing.feature_names) ; df.shape / X = df[['AveRooms','HouseAge']] ; y = housing.target ; X.head() / from sklearn.model_selection import train_test_split ; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 , random_state = 110) / FUNCTIONS== def predict_price(X_train,y_train,sample,p, k=5): ;  distances = [] ;  for x in X_train.to_numpy():  distance =  minkowski_distance(sample,x,p) ;  distances.append(distance) ;  sorted_distances = np.argsort(distances) ;  k_nearest_targets = y_train[sorted_distances[:k]] ;     predict_price = np.mean(k_nearest_targets) ;  return predict_price /  sample1 = X_test.iloc[1,:] ; k=11 ; predict_price(X_train,y_train,k,sample1) / y_test[1] /  y_preds= [] ; for x in X_test.to_numpy(): ;     y_pred = predict_price(X_train,y_train,k,x,3) ;     y_preds.append(y_pred) ;y_preds / from sklearn.metrics import mean_squared_error ; mse = mean_squared_error(y_test,y_preds) ;mse # k=11/  def calculate_mse_for_k(X_train,y_train,X_test,y_test,k,p): ;  y_preds= [] ;    for sample in X_test.to_numpy(): ;   y_pred = ; predict_price(X_train,y_train,sample,k,p) ;    y_preds.append(y_pred) ;    mse = mean_squared_error(y_test,y_preds) ;    return mse/ k_values = range(3,30) ; mse_values= [] ; for k in k_values:  ;      mse = calculate_mse_for_k(X_train,y_train,X_test,y_test,k,p=3) ;     mse_values.append(mse) ; mse_values  "
2.KNNREGRESOR Sklearn,"from sklearn.neighbors import KNeighborsRegressor / knn_regressor = KNeighborsRegressor(n_neighbors=11,p=3) / knn_regressor.fit(X_train,y_train) / y_pred = knn_regressor.predict(X_test) / mse = mean_squared_error(y_test,y_pred) / mse  /// //// CALIFORNIA HOUSING from sklearn.datasets import fetch_california_housing / import pandas as pd / import numpy as np / from sklearn.neighbors import KNeighborsRegressor / from sklearn.metrics import mean_squared_error / housing = fetch_california_housing() / df = pd.DataFrame(housing.data, columns = housing.feature_names) / df.shape / X = df[['AveRooms','HouseAge']] / y = housing.target /X.head() / knn_regressor = KNeighborsRegressor(n_neighbors=11,p=3) / knn_regressor.fit(X_train,y_train) / y_pred = knn_regressor.predict(X_test) / mse = mean_squared_error(y_test,y_pred) / mse"
3.KNNCLASSIFIIER Scratch,"def predict_class(X_train,y_train,sample,p, k=5):  ;     distances = [] ;    for x in X_train.to_numpy():  ;        distance = minkowski_distance(sample,x,p)  ;    distances.append(distance) ;   sorted_distances = np.argsort(distances) ;     k_nearest_targets = y_train[sorted_distances[:k]] ;     class_counts = np.bincount(k_nearest_targets) ;    prediction = agrmax(class_counts) ;     return prediction"
4.KKNCLASSIFIER Sklearn,"IRIS DATA import pandas as pd ; import numpy as np /  Loading data == from sklearn.datasets import load_iris ; from sklearn import neighbors ; from sklearn.model_selection import train_test_split ; from sklearn.neighbors import KNeighborsClassifier ; #features X = load_iris().data print(X) ; #target y = load_iris().target  y / # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25) ; # Create a KNN classifier with k=3 , knn = KNeighborsClassifier(n_neighbors=3) ; # Train the classifier , knn.fit(X_train, y_train) ; # Predict the labels of the test data , y_pred = knn.predict(X_test) ; print(""Prediction"",y_pred) / # Calculate the accuracy of the classifier , accuracy = knn.score(X_test, y_test) / # Print the accuracy
print(""Accuracy:"", accuracy)  LAB_COLORS import pandas as pd / import numpy as np / k=5 / # Data data = { /n  'Color': ['Blue', 'Blue', 'Blue', 'Blue', 'Blue', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange'], /n   'x2': [15, 12, 10, 6, 8, 20, 28, 25, 30, 35], /n    'y2': [13, 10, 17, 15, 25, 10, 13, 5, 10, 14] /n }  / df = pd.DataFrame(data) / black_point = {'x': 19, 'y': 14} / df['euclidean_distance'] = np.sqrt((df['x2'] - black_point['x'])**2 + (df['y2'] - black_point['y'])**2)   /  df_sorted = df.sort_values(by='euclidean_distance')  / df_k = df_sorted.head(k) / df_k['Color'].value_counts(ascending=False).index[0] IRIS DATA WITH FEATURES import pandas as pd / import numpy as np / from sklearn.model_selection import train_test_split / from sklearn.datasets import load_iris / # features X = load_iris().data / # target
y = load_iris().target / X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=12)  (test_size = , default 75-25 ) / from sklearn.neighbors import KNeighborsClassifier/ model1 = KNeighborsClassifier(n_neighbors=5) / model1.fit(X_train, y_train) / print(f""training accuracy:{model1.score(X_train, y_train):.2%}"") / print(f""testing accuracy: {model1.score(X_test, y_test):.2%}"") ////  IRIS DATA WITH FEATURES  "
5.DECISIONTREE Scratch ," 1. LIST 5 INTERS AND SPLIT import numpy as np / from numpy.random import default_rng /  rnd_list = default_rng(110) / int_list = rnd_list.integers(low=5, high=50, size = 5) / int_list //// def splits(x):
    splits_points = []  /    x_sorted = sorted(x)  /     for i in range(len(x)-1):  /      mid_point = (x_sorted[i] + x_sorted[i+1])/2 /       splits_points.append(mid_point) /     return splits_points  //// 2. ARRAY SHAPE 5,3 ITERATE COLUMNS AND FIND SPLIT POINTS.  int_twoDlist = rnd_list.integers(low=5, high=100, size = (5,3))  / int_twoDlist // def splits_per_col(data): /  col_split_pts = {}  /   ncols = data.shape[1] /     for c in range(ncols): /         split_pts = splits(data[:,c]) /        col_split_pts['col_'+ str(c)] = split_pts /     return col_split_pts // splits_per_col(int_twoDlist)  3. ARRAY 5,2 SPLIT POINTS AND MAE FOR THE ROOT int_twoDlist = rnd_list.integers(low=5, high=100, size = (5,2)) /  int_twoDlist  // def split_data(x,y,split_pt): /    mask = x > split_pt /     anti_mask = x < split_pt /    x_true = x[mask] /    y_true = y[mask] /    x_false = x[anti_mask] /    y_false = y[anti_mask] /     return x_true, y_true, x_false , y_false  // x_true, y_true, x_false , y_false =split_data(int_twoDlist[:,0],int_twoDlist[:,1], 70)  //  def MAE(y, y_true, y_false): /     y_true_hat = np.mean(y_true) /     y_false_hat = np.mean(y_false) /     y_hat = np.mean(y) //       old_mae = np.mean(np.absolute(y-y_hat)) /    new_mae = len(y_true)/ len(y) * np.mean(np.absolute(y_true-y_true_hat)) + \       len(y_false)/ len(y) * np.mean(np.absolute(y_false-y_false_hat))  //    return old_mae, new_mae  /// MAE(int_twoDlist[:,1],y_true,y_false) ///  GET THE BEST ESPLIT = def get_best_split(data): /     x = data[:,0] /     y = data[:,1] /     split_pts = splits(x)  /    results = {} /     for point in split_pts: /        x_true, y_true, x_false , y_false = split_data(x,y, point) /     errors = MAE(y, y_true, y_false) /      results[point] = errors  -- next fuera loop- >    best_point = min(results,key=results.get) /   return results,best_point  //// get_best_split(int_twoDlist)  //// MORE THAN 1 FEATURE  def get_best_split_features(data): /   n_feature = data.shape[1] - 1  /  results = {}  /    for feature_idx in range(n_feature): /      x = data[:,feature_idx] /     y = data[:,-1]  /  split_pts = splits(x) --- Loop insde the loop indent-->      for point in split_pts: /      x_true, y_true, x_false , y_false = split_data(x,y, point)  /    errors = MAE(y, y_true, y_false) /   results[feature_idx,point] = errors / --indente al primer loop --->  best_point = min(results,key=results.get)  /   return results,best_point"
6.DECISIONTREESKLEARN,"from sklearn.tree import DecisionTreeRegressor  # code to create a decision tree for regression / from sklearn import tree      # code to visualize a decision tree / import matplotlib.pyplot as plt     # plotting library needed to draw the nodes and branches/  import pandas as pd    / x = [1.5, 1., 2., 3., 2.5] / y = [1, 1.5, 2.5, 2.5, 3.] / apt = pd.DataFrame({'x':x , ""y"":y}) / apt / X = apt.drop('y', axis =1)   # drops the 'y' column from our dataframe and saves the rest of the columns (here just 'x') in variable X / y = apt['y']    # selects the 'y' column and saves it in the variable y / regr = DecisionTreeRegressor(random_state=1234)  # this creates a generic decision tree model (sort of like a recipe) / model = regr.fit(X, y)  "
7.RANDOMFOREST Sklearn,"import pandas as pd / rent = pd.read_csv('rent-ideal.csv') ; rent.head(8) /  TRAINING X = rent[['bedrooms','bathrooms','latitude','longitude']] ; y = rent['price'] /  X.values y.values /  from sklearn.ensemble import RandomForestRegressor /  rf = RandomForestRegressor(n_estimators=10)  / rf.fit(X, y) / import numpy as np ; unknown_x = np.array([2, 1, 40.7957, -73.97]) ;  unknown_x / rf.predict([unknown_x]) / from sklearn.metrics import mean_absolute_error / predictions = rf.predict(X) / e = mean_absolute_error(y, predictions) / ep = e*100 / y.mean() / print(f""${e:.0f} average error; {ep:.2f}% error"") "
8.DISTANCES,"def euclidean_distance(point1, point2):  return np.sqrt(sum((point1-point2)**2)) / def manhattan_distance(point1, point2):     return np.sum(np.abs(point1-point2)) / def minkowski_distance(point1, point2,p):  return np.power(sum(np.abs(point1-point2)**p),1/p)
    
     
    "
9. RANDOM LISTS,"RANDOM LIST USING UNIFORM/FLOATS ; import numpy as np ; rng = np.random.default_rng(23) ; n2 = rng.uniform(2,20,10) ; n2 //// rng = np.random.default_rng(132) ; y = rng.uniform(2,10,10) ;pred = rng.uniform(2,10,10) -- range 2-10 and give me 10 #   /// INTEGERS import numpy as np * from numpy.random import default_rng * rnd_list = default_rng(110) ** int_list = rnd_list.integers(low=5, high=50, size = 5) ////import numpy as pd
from numpy.random import default_rng

#Create a random list of 5 integers rnd_list = default_rng(110)  #seed / int_twoDlist = rnd_list.integers(low= 5 , high=100 , size = (5,2))
int_twoDlist"
10. METRICS REGRESSION,"MSE def mean_squared_error(actual, predicted): *   error = actual - predicted  *    squared_error = error * error  //     mean_squared_error = squared_error.sum()/len(actual) /   return mean_squared_error  /// MAE def MAE(y,pred):    return np.mean(np.abs(y-pred))  /// R2 = def r_square1(y, pred): *     residual_sum_of_squares = ((y - pred) ** 2).sum()  /     total_sum_of_squares = ((y - y.mean()) ** 2).sum() /    r_squared = 1 - (residual_sum_of_squares / total_sum_of_squares) /     return r_squared //// R2 PROFESSOR def r2(y_true, y_pred): /     numerator = len(y_true) *MSE(y_true, y_pred) /     y_avg= np.mean(y_true) /     diff = y_true - y_avg  /     square = diff**2 /    sum_of_squares = np.sum(square) /     denominator = sum_of_squares /    r2 = 1 - (numerator/denominator) /     return r2 /// SLEARN from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, precision_score, recall_score, r2_score"
11. METRICS CLASSIFICATION," EXAMPLE ; ACCURACY TP + TN / ALL( TP+TN+FP+FN)y   ; y = np.array([0,0,0,0,1,1,1,1]) ; pred = np.array([1,0,1,1,1,1,1,0])  /// tp = np.sum((y == 1) & (pred== 1)) ;  tn = np.sum((y == 0) & (pred== 0)) ; fp = np.sum((y == 1) & (pred== 0)) ; fn = np.sum((y == 0) & (pred== 1)) / def accuracy_score(tn,tp,fn,fp):   acs = (tn + tp) / (tn + tp + fn + fp) /     return acs.mean() //// PRECISSION def precision(tp,fn):     precision = tp / (tp+fn) /     return precision  //// RECALL def recall_score(tp,fp): /     recall = tp / (tp+fp) /     return recall /// F1 SCORE precision = tp / (tp+fn)  // recall_score = tp / (tp+fp)  ; def f1_score(precision, recall): /     f1_score = (2 *precision * recall_score) / (precision + recall_score) /    return f1_score  SKLEARN from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
12. CONFUSION MATRIX,"import numpy as np / from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report / y = np.array([0,0,0,0,1,1,1,1]) / pred = np.array([1,0,1,1,1,1,1,0]) / confusion_matrix(y,pred) / print(classification_report(y,pred)) /  ALL THE OTHER METRICS LIKE THIS recall_score(y,pred)"
13. SPLIT PER COLUMN',"def splits_per_col(data):     col_splits_pts = {}     ncols = data.shape[1]     for c in range(ncols):  IND         splits_pts = splits(data[:,c]) IND          col_splits_pts['col_'+ str(c)] = splits_pts  #key to the columns  RI   return  col_splits_pts      "
14. DT WITH DETPH AND LOOP 10 TIMES,"fun =np.random.default_rng(23) / x_dt= fun.uniform(2,20,10) / y_dt= fun.uniform(2,20,10) /x_dt = np.array(x_dt).reshape(-1, 1): from sklearn.tree import DecisionTreeRegressor /from sklearn.metrics import mean_absolute_error / errors = []    / for i in range(10): /   X_train, X_test, y_train, y_test =   train_test_split(x_dt, y_dt, test_size=0.20) /    dt = DecisionTreeRegressor(max_depth=2) /    dt.fit(X_train, y_train) / y_predicted = dt.predict(X_test) /    e = mean_absolute_error(y_test, y_predicted) /    print(f"" ${e:.0f}"", end='') /    errors.append(e) / RI print() /noisy_avg_mae = np.mean(errors)/print(f""Average validation MAE ${noisy_avg_mae:.0f}"")
"
15. R2 ,"SE_line = sum((y-y_hat)**2)    // SE_mean = sum((y-y.mean())**2) //  r2 = 1-(SE_line/SE_mean)  // print(f""R^2 coefficient of determination: {r2*100:0.2f}%"")"
