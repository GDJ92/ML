module KNN,"from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier"
load data,"fun =np.random.default_rng(23) ||  x_dt= fun.uniform(2,20,10)  || y_dt= fun.uniform(2,20,10) || OPCION: x_dt = np.array(x_dt).reshape(-1, 1)"
split,"from sklearn.model_selection import train_test_split  || x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=110)"
model knn reg,"knn_reg= KNeighborsRegressor(n_neighbors=3,p=2) || knn_reg.fit(x_train, y_train) || preds= knn_reg.predict(x_test) || mean_squared_error(preds,y_test)"
model knn class,"knn_class = KNeighborsClassifier(n_neighbors=3,p=2)|| knn_class.fit(x_train, y_train) || preds= knn_classs.predict(x_test) || accuracy_score(preds,y_test)"
module DT,"from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor"
model DT reg,"dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf= 0.1, random_state=3) || dt.fit(X_train, y_train) # Features and targets ||  y_pred = dt.predict(X_test) || decition_tree.score(X_train, y_train) & decition_tree.score(X_test, y_test) "
model DT class,"clf = DecisionTreeClassifier() || clf = clf.fit(X_train,y_train) || preds = clf.predict(X_test) ||  metrics.accuracy_score(y_test, preds))"
metrics,"from sklearn.metrics import accuracy_score, r2_score, f1_score, recall_score, mean_squared_error,  mean_absolute_error"
10 runs DT,"from sklearn.tree import DecisionTreeRegressor || from sklearn.metrics import mean_absolute_error |*|*|  errors = []    || for i in range(10): ||     X_train, X_test, y_train, y_test = \ ||   train_test_split(x_dt, y_dt, test_size=0.20) ||     dt = DecisionTreeRegressor(max_depth=2) ||   dt.fit(X_train, y_train) ||    y_predicted = dt.predict(X_test) || e = mean_absolute_error(y_test, y_predicted) ||     print(f"" ${e:.0f}"", end='') ||   errors.append(e) || print() || noisy_avg_mae = np.mean(errors) || print(f""Average validation MAE ${noisy_avg_mae:.0f}"")"
manhatan,"def manhatan_funtion(x,y): ||  return sum(np.abs(x-y)) || a= np.array([1,5]) || b= np.array([3,10]) || manhatan_funtion(a,b)"
euclidean,"def euclideann_funtion(x,y): ||  return np.sqrt(sum((x-y)**2)) || euclideann_funtion(a,b)"
mikowski,"def mikowski_funtion(x,y,p): ||   return np.power(sum(np.abs(x-y)**p),1/p)"
MSE,"def mse_2(y,pred): ||  return np.mean((y_test - pred) ** 2)"
MAE,"def mae(y,pred): ||   return  np.mean(np.abs(y _test- pred))"
R2,"def r_sqr(y, pred): ||   test_r = 1 - (np.sum((y-pred)**2))/(np.sum((y-np.mean(y))**2)) ||   return test_r"
"CALCULATING TP,TN,FP,FN","true_positives = np.sum((y == 1) & (pred == 1))  || true_negatives = np.sum((y == 0) & (pred == 0)) || false_positives = np.sum((y == 0) & (pred == 1)) || false_negatives = np.sum((y == 1) & (pred == 0)) || print(f""TP:{true_positives} TN:{true_negatives}, FP: {false_positives}, FN: {false_negatives} "")"
accuracy,"def accuracy(tp,tn,fp,fn): ||   acc = (tp+tn)/(tp+tn+fp+fn) ||   return acc"
PRECISION,"def precision(tp,fp): ||   pres= (tp)/(tp+fp) ||   return pres"
RECALL,"def recall(tp,fn): ||   rec= tp/(tp+fn) ||   return rec"
F1-SCORE,"def f_one(tn,tp,fn): ||   f_one_formula= (2*precision(tn,tp)*recall(tp,fn))/(precision(tn,tp)+recall(tp,fn)) ||  return f_one_formula"
split list(DT),def splits(x): ||     splits_points = [] ||     x_sorted = sorted(x) ||     for i in range(len(x)-1): ||  mid_point = (x_sorted[i] + x_sorted[i+1])/2 ||   splits_points.append(mid_point) ||   return splits_points
split MATRIX (DT),"def splits_per_col(data): ||    col_split_pts = {} ||   ncols = data.shape[1] ||   for c in range(ncols): ||   split_pts = splits(data[:,c]) ||   col_split_pts['col_'+ str(c)] = split_pts ||   return col_split_pts"
Mask=filter (DT),"def split_data(x,y,split_pt): ||   mask = x > split_pt ||   anti_mask = x < split_pt ||   x_true = x[mask] ||   y_true = y[mask] ||   x_false = x[anti_mask] ||    y_false = y[anti_mask] ||  return x_true, y_true, x_false , y_false  |*|*|*|  x_true, y_true, x_false , y_false =split_data(int_twoDlist[:,0],int_twoDlist[:,1], 70) || print(x_true, y_true, x_false , y_false)"
#best k with euclidian (KNN)," errors=[] || for k in range(1,20): ||    error= knn_regr (x_test, y_test,x_train,y_train,2,k=k) ||  errors.append(error) ||OPCION: print(k, error)"
#best p alone (KNN),"errors=[] || for p in range(1,10): ||  error= knn_regr (x_test, y_test,x_train,y_train,p=p,k=6) ||  errors.append(error)"
# the best combination between k and p (KNN),"best_error=1 || for p in range(1,6): ||     for k in range(1,20):   ||         knn_reg= KNeighborsRegressor(n_neighbors=k,p=p) ||         knn_reg.fit(x_train, y_train)||         preds= knn_reg.predict(x_test) ||         error=mean_squared_error(preds,y_test)||        if error < best_error: ||            best_error=error||             best_k=k ||            best_p=p |*|*| print(best_error, best_k, best_p)"
Scratch numpy KNN_clasif,"from sklearn.metrics import accuracy_score || def mikowski_funtion(x,y,p): ||     return np.power(sum(np.abs(x-y)**p),1/p) |*|  def distance_fun(sample,x_train,y_train,p,k): ||     distances= [] ||     for i in range(len(x_train)): ||     distance= mikowski_funtion(sample,x_train[i,:],p) ||         distances.append(distance) ||     idx= np.argsort(distances) #argsort is for indexes||     pred = np.argmax(np.bincount(y_train[idx][:k])) ||     return pred |*| def knn_class (x_test, y_test,x_train,y_train,p,k): ||     preds= [] ||     for s in range(len(x_test)): ||   pred = distance_fun(x_test[s],x_train,y_train,p,k) ||         preds.append(pred) ||     return accuracy_score(preds,y_test) |*|  knn_class (x_test, y_test,x_train,y_train,2,3)"
Scratch Pandas KNN_clasif,"from sklearn.metrics import accuracy_score || def mikowski_funtion(x,y,p): ||     return np.power(sum(np.abs(x-y)**p),1/p)  |*| def distance_fun(sample,x_train,y_train,p,k): ||     distances= [] ||   for i in range(len(x_train)): ||     distance= mikowski_funtion(sample,x_train.iloc[i,:],p)||       distances.append(distance)||    idx= np.argsort(distances) #argsort is for indexes ||  pred = np.argmax(np.bincount(y_train.iloc[idx][:k])) ||   return pred  |*|  def knn_class (x_test, y_test,x_train,y_train,p,k): ||  preds= [] ||   for s in range(len(x_test)): ||  pred = distance_fun(x_test.iloc[s],x_train,y_train,p,k) ||    preds.append(pred) ||     return accuracy_score(preds,y_test) |*| knn_class (x_test, y_test,x_train,y_train,2,3"
Scratch Pandas KNN_regression,"def knn_regr (x_test, y_test,x_train,y_train,p,k): ||     preds= [] ||    for s in range(len(x_test)): ||         pred = distance_fun(x_test.iloc[s],x_train,y_train,p,k) ||       preds.append(pred) ||     return mean_absolute_error(preds,y_test)"
numpy to pandas,"x = pd.DataFrame(housing.data,columns= housing.feature_names) & y = pd.DataFrame(housing.target, columns = ['price'])"
manipulate pd and np,"y= y.iloc[:100]  &  x=x[['HouseAge','AveRooms']]  | =VS=| a= housing.data || a[:100]"
load data way 2,"from numpy.random import default_rng || rnd_list = default_rng(110) || int_list = rnd_list.integers(low=5, high=50, size = 5)  |*|*|   int_twoDlist = rnd_list.integers(low=5, high=100, size = (5,3)) #MATRIX"
load data way 3,"p.random.seed(12) || np.random.randint(20, 40, 100) |*OR*|  rag = np.random.default_rng(1234) || y = rag.uniform(20,40,10) ||pred = rag.uniform(20,40,10)"
